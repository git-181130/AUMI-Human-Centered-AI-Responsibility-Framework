# AUMI — Human-Centred AI Responsibility Framework

## What AUMI Is

AUMI is a comprehensive framework for designing, evaluating, governing, and operating human-facing AI systems responsibly.

It exists to address a growing gap in modern AI:

AI systems can be highly capable and technically correct,  
yet still cause harm through how they behave with humans over time.

AUMI defines responsibility not as policy compliance or refusal logic, but as a **system-level obligation** spanning:

- Behaviour  
- Architecture  
- Companionship  
- Operations  
- Organizational decision-making

---

## Why AUMI Exists

As AI systems become:

- Conversational  
- Persistent  
- Emotionally aware  
- Memory-enabled  
- Embedded in daily decision-making  

They stop being experienced as tools and start being experienced as **presences**.

Most existing AI frameworks focus on:

- Accuracy  
- Alignment  
- Content moderation  
- Short-term safety  

These approaches fail to address:

- Subtle emotional harm  
- Dependency formation  
- Role confusion  
- Long-term trust erosion  
- Post-deployment degradation  
- Organizational pressure overriding responsibility  

AUMI exists to close that gap.

---

## What Problem AUMI Solves

AUMI addresses failures that:

- Pass policy checks  
- Pass offline evaluations  
- Appear compliant  
- Yet still harm users over time  

Examples include:

- Over-helpfulness that reduces human agency  
- Emotionally warm responses that create dependency  
- Correct advice delivered unsafely  
- Inconsistent behaviour across time or updates  
- Responsibility degrading after launch  

These failures are **systemic, not edge cases**.

---

## What AUMI Is Not

AUMI is not:

- An AI model  
- A chatbot personality  
- A prompt framework  
- A compliance checklist  
- A legal or regulatory substitute  

AUMI is a responsibility framework that governs how AI systems behave, evolve, and are operated in human contexts.

---

## The AUMI Framework (High Level)

AUMI is structured as a **five-layer responsibility stack**, each layer building on the previous one.

### v0.9 — Foundational Vision
Defines why human-centred AI responsibility is necessary.

### v1 — Behavioral Responsibility
Defines what responsible AI behaviour looks like in human interaction.

### v2 — Technical Architecture & Interaction Design
Defines how responsibility is enforced structurally inside AI systems.

### v3 — Human Companion Layer
Defines how long-term human interaction, emotion, memory, and dependency are governed.

### v4 — EvalOps & Safety Governance
Defines how AI behaviour is evaluated, monitored, governed, and defended in production.

### v5 — Business, Product & Organizational Execution
Defines how responsibility survives scale, growth pressure, and real business constraints.

Together, these layers form a full lifecycle model for responsible AI systems.

---

## Repository Structure

This repository presents a public, high-level view of the AUMI framework.

AUMI-Human-centered-AI-Framework/
├── README.md ← Master overview
├── 00_Executive_Overview/ ← v0.9 Vision & framing
├── 01_AUMI_v1_Behavioral_Responsibility/
├── 02_AUMI_v2_Technical_Architecture/
├── 03_AUMI_v3_Human_Companion_Layer/
├── 04_AUMI_v4_EvalOps_and_Governance/
├── 05_AUMI_v5_Business_Product_Collaboration/
├── 06_Real_World_Application/ ← ZomoBot case study
├── LICENSE
└── CONTRIBUTING.md

Each folder contains:

- A concise `README.md` explaining the layer  
- A link to a PDF with deeper documentation  

---

## About the PDFs

The linked PDFs provide deeper framework detail while intentionally abstracting:

- Internal SOPs  
- Operational thresholds  
- Proprietary metrics  
- Real incident logs  

This separation reflects real-world governance practice.

---

## Real-World Application

AUMI is not theoretical.

The framework has been validated through a real AI system (ZomoBot), including:

- Hallucination incidents  
- Behavioural failures  
- EvalOps gaps  
- Incident → RCA → Postmortem loops  

The case study demonstrates how AUMI operates under realistic constraints.

---

## Who This Repository Is For

This repository is intended for:

- AI product leaders and founders  
- AI safety, trust, and governance teams  
- EvalOps and AI product operations professionals  
- Designers and engineers building human-facing AI  
- Organizations deploying AI at scale  

It is especially relevant for trust-sensitive domains.

---

## How to Use This Repository

Use this repository to:

- Understand the full scope of AI responsibility beyond accuracy  
- Align teams on shared responsibility language  
- Evaluate gaps in existing AI systems  
- Inform product, architecture, and governance decisions  
- Support interviews, audits, and leadership discussions  

This is a reference framework, not a quick-start guide.

---

## Status

- **Framework Status:** Complete (v0.9 → v5.0)  
- **Orientation:** Human-centred, System-Level Responsibility  
- **Operational Readiness:** Depends on layer adoption  
- **Maintenance:** Iterative, evidence-driven  

---

## Closing Note

AUMI is built on a simple but non-negotiable belief:

As AI systems become more present in human life,  
responsibility must become explicit, continuous, and owned.

This repository exists to make that responsibility:

- Designable  
- Enforceable  
- Scalable  

---

## [Executive Master Overview](https://drive.google.com/file/d/1iSzJPCXvfkAcLEbgoncbacFYuHsHvxKH/view?usp=drive_link)
